{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from models.logistic_regression import LogisticRegression\n",
    "\n",
    "\n",
    "def create_features_for_poly(X, degree):\n",
    "    \"\"\"\n",
    "    Expands input features to the specified polynomial degree.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Input matrix of shape (n_samples, n_features).\n",
    "    degree (int): Degree of the polynomial features.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Expanded features of shape (n_samples, n_poly_features).\n",
    "    \"\"\"\n",
    "    if degree < 1:\n",
    "        raise ValueError(\"Degree must be at least 1.\")\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    features_poly = X\n",
    "\n",
    "    for d in range(2, degree + 1):\n",
    "        features_poly = np.concatenate([features_poly, X**d], axis=1)\n",
    "\n",
    "    return features_poly\n",
    "\n",
    "def mean_logloss(y_real, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the mean log loss for a binary classification problem.\n",
    "\n",
    "    Parameters:\n",
    "    y_real (numpy.ndarray): True labels, shape (n_samples,).\n",
    "    y_pred (numpy.ndarray): Predicted probabilities, shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "    float: Mean log loss.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # To avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_real * np.log(y_pred) + (1 - y_real) * np.log(1 - y_pred))\n",
    "\n",
    "def classif_error(y_real, y_pred):\n",
    "    \"\"\"\n",
    "    Computes classification error percentage.\n",
    "\n",
    "    Parameters:\n",
    "    y_real (numpy.ndarray): True labels, shape (n_samples,).\n",
    "    y_pred (numpy.ndarray): Predicted labels, shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "    float: Classification error percentage.\n",
    "    \"\"\"\n",
    "    incorrect = np.sum(y_real != y_pred)\n",
    "    return (incorrect / len(y_real)) * 100\n",
    "\n",
    "\n",
    "def plot_decision_boundary(X, y, theta, degree):\n",
    "    \"\"\"\n",
    "    Plots decision boundary for a logistic regression model with polynomial features.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Input matrix of shape (n_samples, n_features).\n",
    "    y (numpy.ndarray): True labels, shape (n_samples,).\n",
    "    theta (numpy.ndarray): Optimized parameters.\n",
    "    degree (int): Degree of polynomial features.\n",
    "    \"\"\"\n",
    "    # Generate a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Create polynomial features for the grid\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_poly = create_features_for_poly(grid, degree)\n",
    "    grid_poly = np.concatenate((grid_poly, np.ones((grid_poly.shape[0], 1))), axis=1)  # Add bias\n",
    "\n",
    "    # Predict probabilities\n",
    "    probs = LogisticRegression(grid_poly, theta).reshape(xx.shape)\n",
    "\n",
    "    # Plot\n",
    "    plt.contourf(xx, yy, probs, levels=[0, 0.5, 1], cmap=\"coolwarm\", alpha=0.6)\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='red', label=\"Class 1\")\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='blue', label=\"Class 0\")\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def shuffle_data(X, y):\n",
    "    \"\"\"\n",
    "    Randomly shuffles the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Features matrix, shape (n_samples, n_features).\n",
    "    y (numpy.ndarray): Labels vector, shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "    tuple: Shuffled X and y.\n",
    "    \"\"\"\n",
    "    permutation = np.random.permutation(len(y))\n",
    "    return X[permutation], y[permutation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Main Orchestration Script for 4 Task\n",
    "# Author: AbdoAllah Mohammad\n",
    "# Description: Combines all modules to complete tasks from pages 7-10 of the B1 project.\n",
    "###############################################\n",
    "\n",
    "# Import dependencies\n",
    "import numpy as np\n",
    "from data.create_data import create_data\n",
    "from models.gradient_descent import GradientDescent\n",
    "from main_project_skeleton import create_features_for_poly, evaluate_loss, classif_error, plot_data, plot_decision_boundary\n",
    "from models.logistic_regression import train_logistic_regression, predict_logistic_regression\n",
    "from utils.utilities import (\n",
    "    evaluate_loss,\n",
    "    evaluate_classification_error,\n",
    "    plot_data,\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_iterations = 1000\n",
    "degree_poly = 2  # Polynomial degree for feature expansion\n",
    "n_samples_train = 400\n",
    "n_samples_val = 4000\n",
    "\n",
    "# ---- Main Workflow ----\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting main pipeline...\")\n",
    "\n",
    "    # Step 1: Generate Data\n",
    "    print(\"Generating training and validation data...\")\n",
    "    X_train, class_labels_train = create_data(n_samples_train)\n",
    "    y_train = (class_labels_train == 1) * 0 + (class_labels_train == 2) * 1\n",
    "\n",
    "    X_val, class_labels_val = create_data(n_samples_val)\n",
    "    y_val = (class_labels_val == 1) * 0 + (class_labels_val == 2) * 1\n",
    "\n",
    "    # Step 2: Polynomial Feature Expansion\n",
    "    print(f\"Expanding features to degree {degree_poly}...\")\n",
    "    X_train_poly = create_features_for_poly(X_train, degree_poly)\n",
    "    X_train_poly = np.concatenate((X_train_poly, np.ones((n_samples_train, 1))), axis=1)\n",
    "\n",
    "    X_val_poly = create_features_for_poly(X_val, degree_poly)\n",
    "    X_val_poly = np.concatenate((X_val_poly, np.ones((n_samples_val, 1))), axis=1)\n",
    "\n",
    "    # Step 3: Train Logistic Regression Model\n",
    "    print(\"Training logistic regression model using gradient descent...\")\n",
    "    theta_opt = train_logistic_regression(\n",
    "        X_train_poly, y_train, learning_rate, num_iterations\n",
    "    )\n",
    "\n",
    "    # Step 4: Evaluate the Model\n",
    "    print(\"Evaluating model performance...\")\n",
    "    y_pred_train = LogisticRegression(X_train_poly, theta_opt)\n",
    "    y_pred_val = LogisticRegression(X_val_poly, theta_opt)\n",
    "\n",
    "    loss_train = evaluate_loss(X_train_poly, y_train, theta_opt)\n",
    "    loss_val = evaluate_loss(X_val_poly, y_val, theta_opt)\n",
    "\n",
    "    error_train = evaluate_classification_error(y_train, y_pred_train)\n",
    "    error_val = evaluate_classification_error(y_val, y_pred_val)\n",
    "\n",
    "    print(f\"Training Loss: {loss_train:.4f}, Validation Loss: {loss_val:.4f}\")\n",
    "    print(f\"Training Error: {error_train:.2f}%, Validation Error: {error_val:.2f}%\")\n",
    "\n",
    "    # Step 5: Plot Data and Decision Boundary\n",
    "    print(\"Plotting data and decision boundaries...\")\n",
    "    plot_data(X_train, class_labels_train)\n",
    "    plot_decision_boundary(X_train, class_labels_train, theta_opt, degree_poly)\n",
    "\n",
    "    plot_data(X_val, class_labels_val)\n",
    "    plot_decision_boundary(X_val, class_labels_val, theta_opt, degree_poly)\n",
    "\n",
    "    print(\"Pipeline complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from models.logistic_regression import LogisticRegression\n",
    "\n",
    "def create_features_for_poly(X, degree):\n",
    "    \"\"\"\n",
    "    Expands input features to the specified polynomial degree.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Input matrix of shape (n_samples, n_features).\n",
    "    degree (int): Degree of the polynomial features.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Expanded features of shape (n_samples, n_poly_features).\n",
    "    \"\"\"\n",
    "    if degree < 1:\n",
    "        raise ValueError(\"Degree must be at least 1.\")\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    features_poly = X\n",
    "\n",
    "    for d in range(2, degree + 1):\n",
    "        features_poly = np.concatenate([features_poly, X**d], axis=1)\n",
    "\n",
    "    return features_poly\n",
    "\n",
    "def mean_logloss(y_real, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the mean log loss for a binary classification problem.\n",
    "\n",
    "    Parameters:\n",
    "    y_real (numpy.ndarray): True labels, shape (n_samples,).\n",
    "    y_pred (numpy.ndarray): Predicted probabilities, shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "    float: Mean log loss.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # To avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_real * np.log(y_pred) + (1 - y_real) * np.log(1 - y_pred))\n",
    "\n",
    "def classif_error(y_real, y_pred):\n",
    "    \"\"\"\n",
    "    Computes classification error percentage.\n",
    "\n",
    "    Parameters:\n",
    "    y_real (numpy.ndarray): True labels, shape (n_samples,).\n",
    "    y_pred (numpy.ndarray): Predicted labels, shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "    float: Classification error percentage.\n",
    "    \"\"\"\n",
    "    incorrect = np.sum(y_real != y_pred)\n",
    "    return (incorrect / len(y_real)) * 100\n",
    "\n",
    "\n",
    "def plot_decision_boundary(X, y, theta, degree):\n",
    "    \"\"\"\n",
    "    Plots decision boundary for a logistic regression model with polynomial features.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Input matrix of shape (n_samples, n_features).\n",
    "    y (numpy.ndarray): True labels, shape (n_samples,).\n",
    "    theta (numpy.ndarray): Optimized parameters.\n",
    "    degree (int): Degree of polynomial features.\n",
    "    \"\"\"\n",
    "    # Generate a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Create polynomial features for the grid\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_poly = create_features_for_poly(grid, degree)\n",
    "    grid_poly = np.concatenate((grid_poly, np.ones((grid_poly.shape[0], 1))), axis=1)  # Add bias\n",
    "\n",
    "    # Predict probabilities\n",
    "    probs = LogisticRegression(grid_poly, theta).reshape(xx.shape)\n",
    "\n",
    "    # Plot\n",
    "    plt.contourf(xx, yy, probs, levels=[0, 0.5, 1], cmap=\"coolwarm\", alpha=0.6)\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='red', label=\"Class 1\")\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='blue', label=\"Class 0\")\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def shuffle_data(X, y):\n",
    "    \"\"\"\n",
    "    Randomly shuffles the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Features matrix, shape (n_samples, n_features).\n",
    "    y (numpy.ndarray): Labels vector, shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "    tuple: Shuffled X and y.\n",
    "    \"\"\"\n",
    "    permutation = np.random.permutation(len(y))\n",
    "    return X[permutation], y[permutation]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
